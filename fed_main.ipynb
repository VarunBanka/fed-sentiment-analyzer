{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdI3DHKPaZYxaYbnoxNtNg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VarunBanka/fed-sentiment-analyzer/blob/main/fed_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "srFiQbWMVrr8"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.path.abspath(\"..\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download some data from kaggle\n",
        "\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"drlexus/fed-statements-and-minutes\")\n",
        "\n",
        "# Move it to /data/raw\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Kaggle cache location\n",
        "source_dir = Path(\"/root/.cache/kagglehub/datasets/drlexus/fed-statements-and-minutes/versions/3\")\n",
        "\n",
        "dest_dir = Path(\"/content/data/raw\")\n",
        "dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "csv_files = list(source_dir.glob(\"*.csv\"))\n",
        "\n",
        "if not csv_files:\n",
        "    raise FileNotFoundError(\"No CSV files found in Kaggle cache directory.\")\n",
        "\n",
        "for file in csv_files:\n",
        "    dest_file = dest_dir / file.name\n",
        "    shutil.copy(file, dest_file)\n",
        "    print(f\"Copied {file.name} → {dest_file}\")\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y__nyVgIXSev",
        "outputId": "778cf914-05f2-4fd6-9a4b-082f8aebceda"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'fed-statements-and-minutes' dataset.\n",
            "Copied Fed_Scrape-2015-2023.csv → /content/data/raw/Fed_Scrape-2015-2023.csv\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clean the dataset\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/data/raw/Fed_Scrape-2015-2023.csv\")\n",
        "\n",
        "# Normalize column names\n",
        "df.columns = df.columns.str.lower()\n",
        "df.head()\n",
        "\n",
        "grouped = (\n",
        "    df.groupby([\"date\", \"type\"])[\"text\"]\n",
        "    .apply(lambda x: \" \".join(x.astype(str)))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "print(\"Number of reconstructed documents:\", len(grouped))\n",
        "grouped.head()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = \" \".join(text.split())  # remove extra spaces\n",
        "    return text\n",
        "\n",
        "grouped[\"text\"] = grouped[\"text\"].apply(clean_text)\n",
        "\n",
        "grouped[\"doc_type\"] = grouped[\"type\"].map({\n",
        "    0: \"statement\",\n",
        "    1: \"minutes\"\n",
        "})\n",
        "\n",
        "grouped = grouped.drop(columns=[\"type\"])\n",
        "grouped.head()\n",
        "\n",
        "grouped.to_csv(\"/content/data/processed/fomc_processed.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVNHF8o1XS7x",
        "outputId": "b64e9d36-7418-497b-84c5-dc1649c0a32d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reconstructed documents: 378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grouped.iloc[0][\"text\"][:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "vOATVVYNZv6m",
        "outputId": "c3f5bc02-204a-435c-bac5-620e1f2e6491"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Federal Reserve Board and the Federal Open Market Committee on Wednesday released the attached minutes of the Committee meeting held on December 16-17, 2014. A summary of economic projections made by Federal Reserve Board members and Reserve Bank presidents for the meeting is also included as an addendum to these minutes. The minutes for each regularly scheduled meeting of the Committee ordinarily are made available three weeks after the day of the policy decision and subsequently are publis'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from pathlib import Path\n",
        "\n",
        "RSS_URL = \"https://www.federalreserve.gov/feeds/speeches.xml\"\n",
        "OUTPUT_FILE = Path(\"data/raw/fed_speeches_raw.csv\")\n",
        "\n",
        "\n",
        "def scrape_full_speech(link):\n",
        "    page = requests.get(link)\n",
        "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
        "\n",
        "    containers = [\n",
        "        soup.find(\"div\", id=\"article\"),\n",
        "        soup.find(\"div\", class_=\"col-xs-12 col-sm-8 col-md-8\"),\n",
        "        soup.find(\"article\")\n",
        "    ]\n",
        "\n",
        "    for container in containers:\n",
        "        if container:\n",
        "            paragraphs = container.find_all(\"p\")\n",
        "            text = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
        "            if len(text) > 1000:\n",
        "                return text\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def scrape_speeches_from_rss():\n",
        "    print(\"Fetching RSS feed...\")\n",
        "    r = requests.get(RSS_URL)\n",
        "    soup = BeautifulSoup(r.content, \"xml\")\n",
        "\n",
        "    items = soup.find_all(\"item\")\n",
        "    print(f\"Found {len(items)} speeches\")\n",
        "\n",
        "    records = []\n",
        "\n",
        "    for item in items:\n",
        "        link = item.link.text\n",
        "        date = item.pubDate.text\n",
        "\n",
        "        print(\"Scraping:\", link)\n",
        "\n",
        "        text = scrape_full_speech(link)\n",
        "\n",
        "        if text:\n",
        "            records.append({\n",
        "                \"date\": date,\n",
        "                \"text\": text,\n",
        "                \"doc_type\": \"speech\",\n",
        "                \"source_url\": link\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "    print(f\"Saved {len(df)} speeches\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_speeches_from_rss()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7HGgVehZ12N",
        "outputId": "a472ec4a-96a1-4057-edec-b9dba793039a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching RSS feed...\n",
            "Found 15 speeches\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/jefferson20260206a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/cook20260204a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/bowman20260130a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/waller20260130a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/jefferson20260116a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/bowman20260116a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/miran20260114a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/powell20260111a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/bowman20260107a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/miran20251215a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/powell20251201a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/jefferson20251121a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/barr20251121a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/cook20251120a.htm\n",
            "Scraping: https://www.federalreserve.gov/newsevents/speech/miran20251119a.htm\n",
            "Saved 15 speeches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GYj0yQNqat-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}